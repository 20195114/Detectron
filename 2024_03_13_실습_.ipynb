{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2HQiBc+UsNzs92m/2kvhb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/20195114/Detectron/blob/main/2024_03_13_%EC%8B%A4%EC%8A%B5_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "gyvUr9qHDh1M"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "# 공통 코드\n",
        "# 차원 축소\n",
        "# 공통 코드\n",
        "import sys\n",
        "# sklearn ≥0.20 필수\n",
        "import sklearn\n",
        "# 공통 모듈 임포트\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
        "np.random.seed(42)\n",
        "# 깔끔한 그래프 출력을 위해\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "import platform\n",
        "from matplotlib import font_manager, rc\n",
        "#매킨토시의 경우\n",
        "if platform.system() == 'Darwin':\n",
        "    rc('font', family='AppleGothic')\n",
        "#윈도우의 경우\n",
        "elif platform.system() == 'Windows':\n",
        "    font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
        "    rc('font', family=font_name)\n",
        "mpl.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 그림을 저장할 위치\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"dim_reduction\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"그림 저장:\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "# 차원 축소\n",
        "# 공통 코드\n",
        "import sys\n",
        "# sklearn ≥0.20 필수\n",
        "import sklearn\n",
        "# 공통 모듈 임\n",
        "from sklearn.datasets import make_blobs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트 전처리\n",
        "# 문장 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "test_sample = '안녕하세요. 반갑습니다. 어서오세요! 환영합니다.'\n",
        "#. 나 !, ? 처럼 문장의 끝은 나타내는 기호나 \\n 단위로 분할해서 list 로 리턴\n",
        "sentences = sent_tokenize(text = test_sample)\n",
        "print(sentences)\n",
        "print(type(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIIeUVaaY8jI",
        "outputId": "571a1fa3-d5b6-4a92-f3dc-d26147be33e0"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요.', '반갑습니다.', '어서오세요!', '환영합니다.']\n",
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \" 나는 아침에 정치 뉴스 와 스포츠 뉴스를 읽어야 합니다.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k-Hl9imZyw0",
        "outputId": "2f51d129-5dec-47e4-f150-2bed46e5d6d6"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나는', '아침에', '정치', '뉴스', '와', '스포츠', '뉴스를', '읽어야', '합니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서를 받아서 문장 단위로 단어 토큰화를 수행해서 리턴하는 함수\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokenize"
      ],
      "metadata": {
        "id": "4DjOJbJOb3Tz"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = tokenize_text(test_sample)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UzA-tGycOOv",
        "outputId": "c21ec54e-c640-4a45-d498-a70b9565c403"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function word_tokenize at 0x792fbb07f6d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 영문 불용어 제거\n",
        "sample = 'The Matrix is everywhere its all around us here even in this room'"
      ],
      "metadata": {
        "id": "_tJcJ9eyemtx"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk 의 불용어를 확인\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaRdQiyAe3TU",
        "outputId": "352849ee-1cb7-4fd0-a831-f1f2dd2ae015"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_word = nltk.corpus.stopwords.words('english')\n",
        "temp = word_tokenize(sample)\n",
        "# 영문을 사용할 떄는 대소문자에 대한 부분을 고려\n",
        "result = [word for word in temp if word.lower() not in stop_word]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JIjb3SEfZRl",
        "outputId": "7ce7007c-1f0e-4af3-c03f-751892ec485c"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Matrix', 'everywhere', 'around', 'us', 'even', 'room']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "# 문장 토큰화 - sentences\n",
        "print(kkma.sentences('안녕하세요!, 반갑습니다.'))\n",
        "# 단어 토큰화(명사만 추출) - nouns\n",
        "print(kkma.nouns('한국어의 단어별 분석'))\n",
        "\n",
        "# 품사 와 함께 추출 - pos(Hannanum 은 morphs 도 제공)\n",
        "print(kkma.pos('한국어의 단어별 분석'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ElhQrxqgPSD",
        "outputId": "51060556-7984-481e-f11f-37c877f953c3"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['안녕하세요!,', '반갑습니다.']\n",
            "['한국어', '단어', '분석']\n",
            "[('한국어', 'NNG'), ('의', 'JKG'), ('단어', 'NNG'), ('별', 'XSN'), ('분석', 'NNG')]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install konlpy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQDgC_umibIz",
        "outputId": "5be7e735-a7b7-4e79-cfcd-3580e1ce337f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6VmaiYqpiXHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 희소 행렬 표현 방식\n",
        "# COO 방식 - 0이 아닌 데이터의 좌표를 기록하는 방식\n",
        "dense = np.array([(3, 0), (0, 2), (1, 0)])\n",
        "# 0이 아닌 데이터의 가중치와 행 과 열 변호를 기록\n",
        "\n",
        "from scipy import sparse\n",
        "#coo 방식으로 희소행렬을 표현\n",
        "sparse_coo = sparse.coo_matrix(([3, 1, 2], (np.array([0,0,1]), np.array([0,2,1]))))\n",
        "print(sparse_coo)\n",
        "# 복원\n",
        "print(sparse_coo.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGvZbQ5couL7",
        "outputId": "c8e47d4f-b898-4b09-f2d7-d8886904551e"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t3\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n",
            "[[3 0 1]\n",
            " [0 2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CRS 방식 - 동일한 행 인텍스를 2번\n",
        "dense = np.array([\n",
        "[0,0,1,0,0,5],\n",
        "[1,4,0,3,2,5],\n",
        "[0,6,0,3,0,0],\n",
        "[2,0,0,0,0,0],\n",
        "[0,0,0,7,0,8],\n",
        "[1,0,0,0,0,0],\n",
        "])\n",
        "#0이 아닌 데이터 추출\n",
        "data = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "\n",
        "# 0이 아닌 데이터의 행 위치와 열 위치를 저장\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "\n",
        "# 행 인덱스가 중복되는 경우가 많음\n",
        "# 중복된 행 인덱스를 1번씩만 표현\n",
        "# 각 행 인덱스의 시작 위치를 기억하고 마지막 0이 아닌 데이터의 개수를 설정\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "#csr 방식으로 저장\n",
        "sparse_csr = sparse.csr_matrix((data, col_pos, row_pos_ind))\n",
        "print(sparse_csr)# 희소행렬로 출력\n",
        "print(sparse_csr.toarray()) # 밀집행렬로 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nspIGdzFzybv",
        "outputId": "f7c98a18-469c-40ca-edee-e7d7f513f789"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 2)\t1\n",
            "  (0, 5)\t5\n",
            "  (1, 0)\t1\n",
            "  (1, 1)\t4\n",
            "  (1, 3)\t3\n",
            "  (1, 4)\t2\n",
            "  (1, 5)\t5\n",
            "  (2, 1)\t6\n",
            "  (2, 3)\t3\n",
            "  (3, 0)\t2\n",
            "  (4, 3)\t7\n",
            "  (4, 5)\t8\n",
            "  (5, 0)\t1\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 생성\n",
        "corpus = [\" 코로나로 거리두기와 코로나 상생 지원금 문의입니다.\",\n",
        "          \"지하철 운행 시간과 지하철 요금 문의 입니다.\",\n",
        "          \"지하철 승강장 문의입니다.\",\n",
        "          \"택시 승강장 문의입니다.\"]\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK5T9P81MfuM",
        "outputId": "82a3822a-29be-4a5a-c4f8-9b1b96fbdd95"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' 코로나로 거리두기와 코로나 상생 지원금 문의입니다.', '지하철 운행 시간과 지하철 요금 문의 입니다.', '지하철 승강장 문의입니다.', '택시 승강장 문의입니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cvect = CountVectorizer()\n",
        "# 기존 문자열을 가지고 훈련\n",
        "# 문서에 등장한 모든 단어를 indexing 만 수행\n",
        "cvect.fit(corpus)#corpus 의 문장을 벡터화\n",
        "# 앞의 숫자 0 은 행 번호이고 뒤의 숫자는 단어의 인덱스이고 세번째 숫자가 등장횟수\n",
        "dtm = cvect.transform(corpus)\n",
        "print(dtm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7A0nyH7MzSO",
        "outputId": "05dcb411-ef37-4b7b-8752-2323b9550e50"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 12)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 10)\t2\n",
            "  (2, 2)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 10)\t1\n",
            "  (3, 2)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 13)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어 확인\n",
        "print(cvect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Miow4ytOvmw",
        "outputId": "0bf911a7-a2b7-488e-93d3-85f46d2fb987"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'코로나로': 12, '거리두기와': 0, '코로나': 11, '상생': 3, '지원금': 9, '문의입니다': 2, '지하철': 10, '운행': 7, '시간과': 5, '요금': 6, '문의': 1, '입니다': 8, '승강장': 4, '택시': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = cvect.get_feature_names_out()\n",
        "\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co9FZADrN-E3",
        "outputId": "009fd6ca-40f5-4689-d4ed-161b458ad4a6"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['거리두기와' '문의' '문의입니다' '상생' '승강장' '시간과' '요금' '운행' '입니다' '지원금' '지하철' '코로나'\n",
            " '코로나로' '택시']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장들을 피처화\n",
        "df_dtm = pd.DataFrame(dtm.toarray(), columns= vocab)\n",
        "\n",
        "print(df_dtm)\n",
        "#원 핫 인코딩 과 다른 점은 하나의 행이 0이 아닌 값이 여러 개 있고 0 이나 1이아닌 존재 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG1wrS8jOuOq",
        "outputId": "665ee84c-c05c-4497-d30b-9df9e751e2d4"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   거리두기와  문의  문의입니다  상생  승강장  시간과  요금  운행  입니다  지원금  지하철  코로나  코로나로  택시\n",
            "0      1   0      1   1    0    0   0   0    0    1    0    1     1   0\n",
            "1      0   1      0   0    0    1   1   1    1    0    2    0     0   0\n",
            "2      0   0      1   0    1    0   0   0    0    0    1    0     0   0\n",
            "3      0   0      1   0    1    0   0   0    0    0    0    0     0   1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer\n",
        "# 문장 내에서 여러 번 등장하면 가중치를 부여하고 여러 문장에서 등장하면 패널티를 부과\n",
        "# 데이터 생성\n",
        "corpus = [\"코로나로 거리두기와 코로나 상생 지원금 문의입니다.\",\n",
        "          \"지하철 운행 시간과 지하철 요금 문의입니다.\",\n",
        "          \"지하철 승강장 문의입니다.\",\n",
        "          \"택시 승강장 문의입니다.\"]\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3PDEH_WPWM3",
        "outputId": "1c497dc6-722c-4eef-d040-02c253d4c68e"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['코로나로 거리두기와 코로나 상생 지원금 문의입니다.', '지하철 운행 시간과 지하철 요금 문의입니다.', '지하철 승강장 문의입니다.', '택시 승강장 문의입니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfv = TfidfVectorizer()\n",
        "# 기존 문자열을 가지고 훈련\n",
        "# 문서에 등장한 모든 단어를 indexing 만 수행\n",
        "tfidfv.fit(corpus)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "DnX68xRRQOaK",
        "outputId": "2b6bdc12-8f77-4e33-a402-7df4f00b113a"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidfv.transform(corpus).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ9iDLxpQyzk",
        "outputId": "825fde12-52ac-4c57-c6a8-6fbf00b13299"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.43551105 0.22726773 0.43551105 0.         0.         0.\n",
            "  0.         0.43551105 0.         0.43551105 0.43551105 0.        ]\n",
            " [0.         0.2174587  0.         0.         0.41671408 0.41671408\n",
            "  0.41671408 0.         0.65708434 0.         0.         0.        ]\n",
            " [0.         0.42389674 0.         0.64043405 0.         0.\n",
            "  0.         0.         0.64043405 0.         0.         0.        ]\n",
            " [0.         0.37919167 0.         0.5728925  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.72664149]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tfidfv.get_feature_names_out()\n",
        "\n",
        "df_dtm = pd.DataFrame(tfidfv.transform(corpus).toarray(), columns= vocab)\n",
        "\n",
        "print(df_dtm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBxE8oCgRCLW",
        "outputId": "eb54f63e-cc17-42e4-f31f-dd444c586f0e"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      거리두기와     문의입니다        상생       승강장       시간과        요금        운행  \\\n",
            "0  0.435511  0.227268  0.435511  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.217459  0.000000  0.000000  0.416714  0.416714  0.416714   \n",
            "2  0.000000  0.423897  0.000000  0.640434  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.379192  0.000000  0.572892  0.000000  0.000000  0.000000   \n",
            "\n",
            "        지원금       지하철       코로나      코로나로        택시  \n",
            "0  0.435511  0.000000  0.435511  0.435511  0.000000  \n",
            "1  0.000000  0.657084  0.000000  0.000000  0.000000  \n",
            "2  0.000000  0.640434  0.000000  0.000000  0.000000  \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.726641  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud\n",
        "# 패키지 설치\n",
        "!pip install pytagcloud\n",
        "!pip install pygame\n",
        "!pip install simplejson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tHsAzVxVUaV",
        "outputId": "e50f08f4-3090-44df-9454-4da4448d7432"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytagcloud in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (3.19.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그리고자 하는 단어의 list를 생성\n",
        "nouns = list()\n",
        "nouns.extend([\" 사랑 \" for t in range(4)])\n",
        "nouns.extend([\" 웃기 \" for t in range(8)])\n",
        "nouns.extend([\" 뽀뽀 \" for t in range(6)])\n",
        "nouns.extend([\" 집 \" for t in range(10)])\n",
        "nouns.extend([\" 부산 \" for t in range(10)])"
      ],
      "metadata": {
        "id": "dHAyht1-WyP5"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lis에 존재하는 데이터의 개수 세기\n",
        "import collections\n",
        "\n",
        "li = ['192.168.0.1', '192.168.0.0','192.168.0.1']\n",
        "# 반복이 가능한 자료구조를 대입하면 각 데이터의 개수를 dict로 만들어 줍니다.\n",
        "count = collections.Counter(li)\n",
        "print(count)\n",
        "\n",
        "count = collections.Counter(nouns)\n",
        "print(count)\n",
        "\n",
        "# 필요한 개수만큼 추출\n",
        "tag2 = count.most_common(10)\n",
        "print(tag2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC3u-yAJYG9a",
        "outputId": "767dbe64-936c-4dc0-f4cb-626769f9fbf5"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'192.168.0.1': 2, '192.168.0.0': 1})\n",
            "Counter({' 집 ': 10, ' 부산 ': 10, ' 웃기 ': 8, ' 뽀뽀 ': 6, ' 사랑 ': 4})\n",
            "[(' 집 ', 10), (' 부산 ', 10), (' 웃기 ', 8), (' 뽀뽀 ', 6), (' 사랑 ', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytagcloud\n",
        "#그릴 준비가 됨 - 텍스트 의 색상 과 글자 크기가 결정됨\n",
        "taglist = pytagcloud.make_tags(tag2, maxsize= 50)\n",
        "print(taglist)\n",
        "# 이미지로 만들기\n",
        "pytagcloud.create_tag_image(taglist, '', size = (900, 600), fontname = 'Korean')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "Q5UPpm33YuTn",
        "outputId": "a680bc91-d691-4e84-f64e-db22b91c92a6"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'color': (137, 201, 89), 'size': 73, 'tag': ' 집 '}, {'color': (166, 198, 136), 'size': 73, 'tag': ' 부산 '}, {'color': (142, 89, 124), 'size': 62, 'tag': ' 웃기 '}, {'color': (63, 85, 105), 'size': 50, 'tag': ' 뽀뽀 '}, {'color': (78, 146, 137), 'size': 36, 'tag': ' 사랑 '}]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Invalid font name. Should be one of Nobile, Old Standard TT, Cantarell, Reenie Beanie, Cuprum, Molengo, Neucha, Philosopher, Yanone Kaffeesatz, Cardo, Neuton, Inconsolata, Crimson Text, Josefin Sans, Droid Sans, Lobster, IM Fell DW Pica, Vollkorn, Tangerine, Coustard, PT Sans Regular",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-f92a0bc953b5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 이미지로 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpytagcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tag_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaglist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Korean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytagcloud/__init__.py\u001b[0m in \u001b[0;36mcreate_tag_image\u001b[0;34m(tags, output, size, background, layout, fontname, rectangular)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     sizeRect, tag_store = _draw_cloud(tags,\n\u001b[0m\u001b[1;32m    341\u001b[0m                                       \u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                                       \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytagcloud/__init__.py\u001b[0m in \u001b[0;36m_draw_cloud\u001b[0;34m(tag_list, layout, size, fontname, rectangular)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0marea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mtag_sprite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0marea\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtag_sprite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mtag_sprites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_sprite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytagcloud/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tag, initial_position, fontname)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfont_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_font\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfontname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         self.font = font.Font(os.path.join(FONT_DIR,\n\u001b[1;32m     61\u001b[0m                                            self.font_spec['ttf']),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytagcloud/__init__.py\u001b[0m in \u001b[0;36mload_font\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     raise AttributeError('Invalid font name. Should be one of %s' % \n\u001b[0m\u001b[1;32m    102\u001b[0m                          \", \".join([f['name'] for f in FONT_CACHE]))\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Invalid font name. Should be one of Nobile, Old Standard TT, Cantarell, Reenie Beanie, Cuprum, Molengo, Neucha, Philosopher, Yanone Kaffeesatz, Cardo, Neuton, Inconsolata, Crimson Text, Josefin Sans, Droid Sans, Lobster, IM Fell DW Pica, Vollkorn, Tangerine, Coustard, PT Sans Regular"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHM_Se_1fPyA",
        "outputId": "eb7a84d9-aca3-4fc2-fe84-0cc1962d2f2b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from PIL import Image"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HOzPgk77gbKr"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "source": [
        "mask = np.array(Image.open('/content/love.png'))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oXE4HfuHgbX5"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "source": [
        "# 워드 클라우드를 그릴 이미지 출력\n",
        "from PIL import Image\n",
        "mask = np.array(Image.open('/content/love.png'))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(mask, cmap=plt.cm.gray, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "IOsR52Aagbqr",
        "outputId": "bfaf5be1-c721-491f-f9c8-cbb08161dc15"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "!pip install Pillow"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb_qwv1ZgVi3",
        "outputId": "58463e41-a26b-4d9a-d1c9-03de2cac727c"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\n",
        "for t in range(3) :\n",
        "  text = text + 러스트\n",
        "  \""
      ],
      "metadata": {
        "id": "VGnz2ZsQhysY"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "#워드 클라우드 그릴 준비:\n",
        "# 한글을 출력하고자 하는 경우는 font_path에 한글 폰트 파일 경로를 설정\n",
        "wordcloud = WordCloud(background_color='white', max_words=2000, mask = mask)\n",
        "\n",
        "#메모리에 워드클라우드 만들기\n",
        "wordcloud = wordcloud.generate(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "lb5utSBUggbl",
        "outputId": "82e14688-6951-4575-e5bf-452f1f365109"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "We need at least 1 word to plot a word cloud, got 0.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-162-18864372b403>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#메모리에 워드클라우드 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[1;32m    623\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    412\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_num ="
      ],
      "metadata": {
        "id": "nBVbLsfdugW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파싱한 내용을 저장할 텍스트 파일 생성\n",
        "output_file = open(keyword + \"txt\", 'w', encoding='utf8')\n",
        "\n",
        "# 기사 링크를 한 곳에 저장하기\n",
        "for i in range(page_num)\n",
        "print(page_num)"
      ],
      "metadata": {
        "id": "8rvjL2RNt4nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(page_num):\n",
        "  current_page_num = 1 + i * 10\n",
        "  target_URL = \"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "9PjHMHFaucvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성 분석\n",
        "# 나이브 베이즈 분류기를 이용한 영문 감성 분석\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "train = [\n",
        "    ('I like you', 'pos'),\n",
        "    ('I do not like you', 'neg'),\n",
        "    ('I hate you', 'neg'),\n",
        "    ('I do not hate you', 'pos'),\n",
        "    ('I love you', 'pos'),\n",
        "    ('I do not like Jessica')\n",
        "]\n",
        "\n",
        "\n",
        "# 단어의 집합을 생성\n",
        "\n",
        "#train 데이터의 모든 것을 순차적으로 sentence에 대입한 후\n",
        "#sentence의 첫번째 데이터를 공백을 기준으로 분할해서 word에 순차적으로 할당 한 후\n",
        "# word를 소문자로 변경해서 set으로 생성\n",
        "all_words = set(word.lower()\n",
        "for sentence in train\n",
        "                for word in word_tokenize(sentence[0]))\n",
        "print(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5SbxTw6vFAk",
        "outputId": "20a29000-0412-4e10-d4fb-ea86ba89cef8"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hate', 'you', 'like', 'i', 'not', 'love', 'do'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처 벡터화 수행\n",
        "# train의 모든 데이터를 x에 대입한 후\n",
        "# 튜플을 생성하는데 뒤는 감성을 그대로 저장하고\n",
        "# 앞은 all_words 모든 단어 와 존재 여부를 딕셔너리로 가지고 있습니다.\n",
        "t = [(dict((word, (word in word_tokenize(x[0]))) for word in all_words), x[1]) for x in train]\n",
        "\n",
        "print(t)\n",
        "# 나이브 베이즈 분류기를 이용해서 감성 분석을 하기 위해서는\n",
        "#각 문장이 단어의 존재 여부를 가진 dict와 감성을 나타내는 텍스트 나 숫자의 튜플로 변환이 되어야 합니다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdzdYQNp59KN",
        "outputId": "aa1496b4-4ecc-4351-adb7-d04c71c64651"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[({'hate': False, 'you': True, 'like': True, 'i': False, 'not': False, 'love': False, 'do': False}, 'pos'), ({'hate': False, 'you': True, 'like': True, 'i': False, 'not': True, 'love': False, 'do': True}, 'neg'), ({'hate': True, 'you': True, 'like': False, 'i': False, 'not': False, 'love': False, 'do': False}, 'neg'), ({'hate': True, 'you': True, 'like': False, 'i': False, 'not': True, 'love': False, 'do': True}, 'pos'), ({'hate': False, 'you': True, 'like': False, 'i': False, 'not': False, 'love': True, 'do': False}, 'pos'), ({'hate': False, 'you': False, 'like': False, 'i': False, 'not': False, 'love': False, 'do': False}, ' ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 나이브 베이즈 분류기로 훈련\n",
        "classifier = nltk.NaiveBayesClassifier.train(t)\n",
        "# 각 단어가 감성에 미치는 확률을 확인\n",
        "classifier.show_most_informative_features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF0eZRdB7Grm",
        "outputId": "f0b73af5-b480-4092-d581-7c1d5ad7dd7d"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                      do = False                 : neg    =      1.5 : 1.0\n",
            "                    hate = False                 : neg    =      1.5 : 1.0\n",
            "                    like = False                 : neg    =      1.5 : 1.0\n",
            "                     not = False                 : neg    =      1.5 : 1.0\n",
            "                      do = True              neg : pos    =      1.3 : 1.0\n",
            "                    hate = True              neg : pos    =      1.3 : 1.0\n",
            "                    like = True              neg : pos    =      1.3 : 1.0\n",
            "                    love = False             neg : pos    =      1.3 : 1.0\n",
            "                     not = True              neg : pos    =      1.3 : 1.0\n",
            "                     you = True              pos : neg    =      1.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 - 예측할 때 문장을 바로 대입하는 것이 아니고 문장을 피처로 변환해서 대입\n",
        "test_sentence = 'I do not like Jessica'\n",
        "\n",
        "test_sent_features = {word.lower(): (word in word_tokenize(test_sentence.lower())) for word in all_words}\n",
        "# 테스트 문장을 피처화\n",
        "print(test_sent_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZTekMgc7R3I",
        "outputId": "a3f4706e-ab47-4805-f96e-ec852cc7dbbd"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hate': False, 'you': False, 'like': True, 'i': True, 'not': True, 'love': False, 'do': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.classify(test_sent_features))\n",
        "# 이런 형태의 애플리케이선을 만들 때는 새로 들어오는 데이터를 가지고\n",
        "# 자동으로 학습하는 형태로 애프릴케이션을 구성해야 합니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-CiEkhS8rb_",
        "outputId": "76e334f9-6e20-4ed9-e141-4368aed1e716"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y_iUJNUh86tp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}